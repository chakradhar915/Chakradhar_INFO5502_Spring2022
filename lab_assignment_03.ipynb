{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chakradhar915/Chakradhar_INFO5502_Spring2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmGi1SuQOgiG"
      },
      "source": [
        "## The third Lab-assignment (02/10/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVozcjmaOgiK"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daRBuk9OOgiL"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "The domain probelm is to collect the data from flipkart by using web scraping\n",
        "reasearch problem is to find to analyse the data to check the discount of the mobile with respect to the cost  \n",
        "\n",
        "Data Collection:- I have collected the data from flipkart of the mobile by using web scraping\n",
        "I have collected 1000 rows of data from flipkart by using web scraping \n",
        "\n",
        "Setps:- \n",
        "\n",
        "By using Web scraping in python \n",
        "by using beautifulsoup we get the html tags from the site\n",
        "we collected the data from the flipkart by using the html tags from the site\n",
        "now we store in the data in csv file which further used for analysis\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "riG4PLr0UvkW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c11f0947-5b59-4868-ee98-0c776c261011"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThe domain probelm is to collect the data from flipkart by using web scraping\\nreasearch problem is to find to analyse the data to check the discount of the mobile with respect to the cost  \\n\\nData Collection:- I have collected the data from flipkart of the mobile by using web scraping\\nI have collected 1000 rows of data from flipkart by using web scraping \\n\\nSetps:- \\n\\nBy using Web scraping in python \\nby using beautifulsoup we get the html tags from the site\\nwe collected the data from the flipkart by using the html tags from the site\\nnow we store in the data in csv file which further used for analysis\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgbnqOLkOgiN"
      },
      "source": [
        "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "vyQyJCX3OgiO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "491f4dce-df49-4390-d8f1-2207f5c2c73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 data collected\n",
            "Page 2 data collected\n",
            "Page 3 data collected\n",
            "Page 4 data collected\n",
            "Page 5 data collected\n",
            "Page 6 data collected\n",
            "Page 7 data collected\n",
            "Page 8 data collected\n",
            "Page 9 data collected\n",
            "Page 10 data collected\n",
            "Page 11 data collected\n",
            "Page 12 data collected\n",
            "Page 13 data collected\n",
            "Page 14 data collected\n",
            "Page 15 data collected\n",
            "Page 16 data collected\n",
            "Page 17 data collected\n",
            "Page 18 data collected\n",
            "Page 19 data collected\n",
            "Page 20 data collected\n",
            "Page 21 data collected\n",
            "Page 22 data collected\n",
            "Page 23 data collected\n",
            "Page 24 data collected\n",
            "Page 25 data collected\n",
            "Page 26 data collected\n",
            "Page 27 data collected\n",
            "Page 28 data collected\n",
            "Page 29 data collected\n",
            "Page 30 data collected\n",
            "Page 31 data collected\n",
            "Page 32 data collected\n",
            "Page 33 data collected\n",
            "Page 34 data collected\n",
            "Page 35 data collected\n",
            "Page 36 data collected\n",
            "Page 37 data collected\n",
            "Page 38 data collected\n",
            "Page 39 data collected\n",
            "Page 40 data collected\n",
            "Page 41 data collected\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    ProductName Discount on mobile  \\\n",
              "0                realme C31 (Dark Green, 64 GB)            16% off   \n",
              "1              realme C31 (Light Silver, 64 GB)            16% off   \n",
              "2                realme C31 (Dark Green, 32 GB)            18% off   \n",
              "3              REDMI 10 (Midnight Black, 64 GB)            26% off   \n",
              "4                REDMI 10 (Pacific Blue, 64 GB)            26% off   \n",
              "..                                          ...                ...   \n",
              "955  SAMSUNG Galaxy F42 5G (Matte Aqua, 128 GB)            11% off   \n",
              "956         Infinix Smart 4 (Ocean Wave, 32 GB)                NaN   \n",
              "957  REDMI Note 10 Pro (Vintage Bronze, 128 GB)             4% off   \n",
              "958          APPLE iPhone 12 Mini (Red, 128 GB)            14% off   \n",
              "959                            KARBONN K140 Pop                NaN   \n",
              "\n",
              "    Offer Price Rating  PageNo  \n",
              "0         9,999    4.6       1  \n",
              "1         9,999    4.6       1  \n",
              "2         8,999    4.7       1  \n",
              "3        10,999    4.5       1  \n",
              "4        10,999    4.5       1  \n",
              "..          ...    ...     ...  \n",
              "955      22,999    4.1      41  \n",
              "956       8,999    4.4      41  \n",
              "957      20,480    4.3      41  \n",
              "958      55,199    4.5      41  \n",
              "959         959    3.9      41  \n",
              "\n",
              "[960 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c02496f-560c-4bd0-bd9f-efe60beba662\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ProductName</th>\n",
              "      <th>Discount on mobile</th>\n",
              "      <th>Offer Price</th>\n",
              "      <th>Rating</th>\n",
              "      <th>PageNo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>realme C31 (Dark Green, 64 GB)</td>\n",
              "      <td>16% off</td>\n",
              "      <td>9,999</td>\n",
              "      <td>4.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>realme C31 (Light Silver, 64 GB)</td>\n",
              "      <td>16% off</td>\n",
              "      <td>9,999</td>\n",
              "      <td>4.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>realme C31 (Dark Green, 32 GB)</td>\n",
              "      <td>18% off</td>\n",
              "      <td>8,999</td>\n",
              "      <td>4.7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>REDMI 10 (Midnight Black, 64 GB)</td>\n",
              "      <td>26% off</td>\n",
              "      <td>10,999</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>REDMI 10 (Pacific Blue, 64 GB)</td>\n",
              "      <td>26% off</td>\n",
              "      <td>10,999</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>SAMSUNG Galaxy F42 5G (Matte Aqua, 128 GB)</td>\n",
              "      <td>11% off</td>\n",
              "      <td>22,999</td>\n",
              "      <td>4.1</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>Infinix Smart 4 (Ocean Wave, 32 GB)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8,999</td>\n",
              "      <td>4.4</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>REDMI Note 10 Pro (Vintage Bronze, 128 GB)</td>\n",
              "      <td>4% off</td>\n",
              "      <td>20,480</td>\n",
              "      <td>4.3</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>APPLE iPhone 12 Mini (Red, 128 GB)</td>\n",
              "      <td>14% off</td>\n",
              "      <td>55,199</td>\n",
              "      <td>4.5</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>959</th>\n",
              "      <td>KARBONN K140 Pop</td>\n",
              "      <td>NaN</td>\n",
              "      <td>959</td>\n",
              "      <td>3.9</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>960 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c02496f-560c-4bd0-bd9f-efe60beba662')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c02496f-560c-4bd0-bd9f-efe60beba662 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c02496f-560c-4bd0-bd9f-efe60beba662');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "mobile_name = [] \n",
        "mobile_cost = []\n",
        "mobile_rating = []\n",
        "discount_price = []\n",
        "pg_num = []\n",
        "\n",
        "for i in range(1,42): # go throught the 41 pages\n",
        "    URL = 'https://www.flipkart.com/search?q=mobiles&as=on&as-show=on&otracker=AS_Query_HistoryAutoSuggest_1_3_na_na_na&otracker1=AS_Query_HistoryAutoSuggest_1_3_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=mobiles&requestId=3d480baf-ddf4-4e4b-9822-cc322d19f8ef&as-backfill=on&page={}'.format(i)\n",
        "    page = requests.get(URL)\n",
        "    pagecontent = page.text\n",
        "    soup = BeautifulSoup(pagecontent)\n",
        "    #by using html tag we collect the data\n",
        "    for x in soup.find_all('div', attrs={'class':'_3pLy-c row'}): # get 24 product properties in each page\n",
        "        mobi_name = x.find('div', attrs = {'class':'_4rR01T'})\n",
        "        mobi_cost = x.find('div', attrs = {'class':'_30jeq3 _1_WHN1'})\n",
        "        mobi_rating = x.find('div', attrs = {'class':'_3LWZlK'})\n",
        "        disc_price = x.find('div', attrs = {'class':'_3Ay6Sb'})\n",
        "        \n",
        "        #checing if none value we append Nan else the text\n",
        "        if mobi_name is None:\n",
        "            mobile_name.append(np.NaN)\n",
        "        else:\n",
        "            mobile_name.append(mobi_name.text)\n",
        "            \n",
        "        if mobi_cost is None:\n",
        "            mobile_cost.append(np.NaN)\n",
        "        else:\n",
        "            mobile_cost.append(mobi_cost.text[1:])\n",
        "            \n",
        "        if mobi_rating is None:\n",
        "            mobile_rating.append(np.NaN)\n",
        "        else:\n",
        "            mobile_rating.append(mobi_rating.text)\n",
        "        if disc_price is None:\n",
        "            discount_price.append(np.NaN)\n",
        "        else:\n",
        "            discount_price.append(disc_price.text)\n",
        "        \n",
        "        \n",
        "        pg_num.append(i)\n",
        "        \n",
        "    print(\"Page\",i,\"data collected\")\n",
        "    \n",
        "mobile_df = pd.DataFrame({'ProductName':mobile_name, 'Discount on mobile':discount_price,'Offer Price':mobile_cost, 'Rating': mobile_rating, 'PageNo': pg_num})\n",
        "mobile_df.to_csv('mobile.csv')\n",
        "\n",
        "mobile_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDbj1q0OgiP"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "aAs5jEuWOgiP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4d81a480-e7c3-40a3-9606-e53214ab7cad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBest strategy to clean the data is to remove duplicates\\nwe can modify the spelling mistakes and abnormal capitalization \\nwe have to check for missing data and to make it as null\\nwe can detect the outliers by using the box plot.\\nif we want we can keep the outlier or we can normalize the entire dataset to get the accurate values.\\n\\nI have taken dataset from kaggle \"https://www.kaggle.com/ashishjangra27/geeksforgeeks-articles\"\\nIn the attribute we have \\'last_updated\\' some fields are not in date format so we have removed the rows which are not in date format\\nthere are so many outlier so that if we want to see the values we can keep if there is no drastic change of the values , if we have drastic \\nchange we have to remove the outliers rows or we can normalize the entire dataset to get the accurate values.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "'''\n",
        "Best strategy to clean the data is to remove duplicates\n",
        "we can modify the spelling mistakes and abnormal capitalization \n",
        "we have to check for missing data and to make it as null\n",
        "we can detect the outliers by using the box plot.\n",
        "if we want we can keep the outlier or we can normalize the entire dataset to get the accurate values.\n",
        "\n",
        "I have taken dataset from kaggle \"https://www.kaggle.com/ashishjangra27/geeksforgeeks-articles\"\n",
        "In the attribute we have 'last_updated' some fields are not in date format so we have removed the rows which are not in date format\n",
        "there are so many outlier so that if we want to see the values we can keep if there is no drastic change of the values , if we have drastic \n",
        "change we have to remove the outliers rows or we can normalize the entire dataset to get the accurate values.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7LJ9c3vOgiQ"
      },
      "source": [
        "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
        "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
        "\n",
        "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
        "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
        "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
        "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
        "ten folds. \n",
        "\n",
        "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
        "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
        "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
        "\n",
        "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
        "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
        "\n",
        "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
        "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
        "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
        "\n",
        "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
        "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
        "dataset is used in all our experiments.\n",
        "\n",
        "(The original paper can be download on canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ChVC720kOgic"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import requests\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/sample_data/datasets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWU7r9uFxLZN",
        "outputId": "aadfbd77-3da0-4e88-fd9c-9c2e4ac3bed7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/sample_data/datasets.zip\n",
            "replace datasets/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: datasets/.DS_Store      \n",
            "  inflating: __MACOSX/datasets/._.DS_Store  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-0.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-0.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-1.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-1.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-2.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-2.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-3.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-3.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-4.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-4.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-5.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-5.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-6.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-6.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-7.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-7.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-8.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-8.validation.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.test.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.test.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.train.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.train.txt  \n",
            "  inflating: datasets/AskAPatient/AskAPatient.fold-9.validation.txt  \n",
            "  inflating: __MACOSX/datasets/AskAPatient/._AskAPatient.fold-9.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-0.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-0.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-1.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-1.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-2.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-2.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-3.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-3.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-4.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-4.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-5.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-5.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-6.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-6.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-7.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-7.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-8.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-8.validation.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.test.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.test.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.train.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.train.txt  \n",
            "  inflating: datasets/TwADR-L/TwADR-L.fold-9.validation.txt  \n",
            "  inflating: __MACOSX/datasets/TwADR-L/._TwADR-L.fold-9.validation.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We combined all examples in training, validation and test data from the original data set.\n",
        "#create an empty data frame \n",
        "result_dataframe=  pd.DataFrame()\n",
        "#function for the lower the case of all the data\n",
        "def caseLower(df):\n",
        "    for columns in data_frame.columns:\n",
        "        data_frame[columns] = data_frame[columns].str.lower()\n",
        "    return(df)\n",
        "#function for dopping the all the dublicate values\n",
        "def dropDublicates(df):\n",
        "    df = df.drop_duplicates('phrase-label')\n",
        "    return(df)\n",
        "  #finction for printing the result\n",
        "def printResult(result):\n",
        "    result = result.T\n",
        "    result.columns = ['TwADR-L', 'AskAPatient']\n",
        "    print(result)\n",
        "#Loop for iterating the two different datasets\n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    newset = {}#empty dataset\n",
        "    data_frame = pd.DataFrame()#creating the data frame\n",
        "    for filepath in glob.glob('/content/datasets/{}/*.txt'.format(dataset)):#we are iterating to get the path of two datasets\n",
        "        data = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')#reading the CSV file \n",
        "        data_frame = data_frame.append(data)#commbining the all the data in one dataset\n",
        "        \n",
        "    data_frame = data_frame.reset_index(drop=True)\n",
        "    data_frame['phrase_label'] = data_frame[1] + \"-\" + data_frame[2]\n",
        "    data_frame.columns = ['id', 'label_name', 'phrases_name', 'phrase-label']#defining the columns \n",
        "    data_frame = data_frame.astype({\"id\": str})#changing the ID variable to string\n",
        "\n",
        "    data_frame = caseLower(data_frame)#function call for the lower the case\n",
        "    data_frame = dropDublicates(data_frame)#function call for the dropping the dublicates\n",
        "    \n",
        "    #we are assigning the values to form a table\n",
        "    newset['Unique_phrases'] = str(len(data_frame['phrases_name'].unique()))\n",
        "    newset['Unique_labels'] = str(len(data_frame['label_name'].unique()))\n",
        "    newset['Unique_phrase_label_pairs'] = str(data_frame.shape[0])\n",
        "    data_frame1 = pd.DataFrame(data_frame['phrases_name'].value_counts())\n",
        "    newset['Phrases with multiple labels'] = str(data_frame1[data_frame1['phrases_name'] > 1].shape[0])\n",
        "    newset['Min examples per label'] = str(data_frame['label_name'].value_counts().values.min())\n",
        "    newset['Max examples per label'] = str(data_frame['label_name'].value_counts().values.max())\n",
        "    newset['Avg examples per label'] = round(data_frame['label_name'].value_counts().mean(),2)\n",
        "    result_dataframe = result_dataframe.append(newset, ignore_index=True)\n",
        "printResult(result_dataframe)#function call to print the result\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJAb-jLhTzDl",
        "outputId": "add03e69-ad44-4b44-940b-08443827e7c5"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             TwADR-L AskAPatient\n",
            "Unique_phrases                  2944        4470\n",
            "Unique_labels                   2220        1038\n",
            "Unique_phrase_label_pairs       3157        4507\n",
            "Phrases with multiple labels     173          35\n",
            "Min examples per label             1           1\n",
            "Max examples per label            36         141\n",
            "Avg examples per label          1.42        4.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newset_dataframe = pd.DataFrame()\n",
        "#function defination to store the data which has less than 5 examples\n",
        "def storeDataWhichIsLessThan5(df):\n",
        "    index_list = []\n",
        "    for i in range(df.shape[0]):\n",
        "        if df['label_name'].value_counts()[df.iloc[i]['label_name']] < 5:\n",
        "            index_list.append(i)\n",
        "    dropDataWhichIsLessThan5(index_list,df)#function call to drop the data which is stored\n",
        "#function defination for to drop the data which has less than 5 examples\n",
        "def dropDataWhichIsLessThan5(df,df1):\n",
        "    # we are removing the data lable which are less than 5\n",
        "    df1.drop(df1.index[df], inplace=True)\n",
        "# we iterate through both datasets \n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    data_frame = pd.DataFrame()\n",
        "    for filepath in glob.glob('/content/datasets/{}/*.txt'.format(dataset)):#we are iterating to get the path of two datasets\n",
        "        data = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')#reading the CSV file \n",
        "        data_frame = data_frame.append(data)#commbining the all the data in one dataset\n",
        "\n",
        "\n",
        "    # we combine by the two data set text files\n",
        "    data_frame = data_frame.reset_index(drop=True)\n",
        "    data_frame['phrase_label'] = data_frame[1] + \"-\" + data_frame[2]\n",
        "    data_frame.columns = ['id', 'label_name', 'phrases_name', 'phrase-label']#defining the columns \n",
        "    data_frame = data_frame.astype({\"id\": str})#changing the ID variable to string\n",
        "    data_frame = caseLower(data_frame)#function call for the lower the case\n",
        "\n",
        "    # we are removing the duplicates\n",
        "    data_frame = dropDublicates(data_frame)#changing the ID variable to string\n",
        "    storeDataWhichIsLessThan5(data_frame)#calling function to store data\n",
        "\n",
        "    \n",
        "    newset = {}\n",
        "    # storing newsets of the table in dictionary\n",
        "    newset['Unique_phrases'] = str(len(data_frame['phrases_name'].unique()))\n",
        "    newset['Unique_labels'] = str(len(data_frame['label_name'].unique()))\n",
        "    newset['Unique_phrase_label_pairs'] = str(data_frame.shape[0])\n",
        "    data_frame1 = pd.DataFrame(data_frame['phrases_name'].value_counts())\n",
        "    newset['Phrases with multiple labels'] = str(data_frame1[data_frame1['phrases_name'] > 1].shape[0])\n",
        "    newset['Min examples per label'] = str(data_frame['label_name'].value_counts().values.min())\n",
        "    newset['Max examples per label'] = str(data_frame['label_name'].value_counts().values.max())\n",
        "    newset['Avg examples per label'] = round(data_frame['label_name'].value_counts().mean(), 2)\n",
        "    # appending dictinary to dataframe\n",
        "    newset_dataframe = newset_dataframe.append(newset, ignore_index=True)\n",
        "printResult(newset_dataframe)#function call to print the result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujLeVMVUZEXd",
        "outputId": "d2a2b38c-02c8-48e4-e3cd-f39ee06d4eae"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             TwADR-L AskAPatient\n",
            "Unique_phrases                   616        2665\n",
            "Unique_labels                     76         233\n",
            "Unique_phrase_label_pairs        721        2686\n",
            "Phrases with multiple labels      87          19\n",
            "Min examples per label             5           5\n",
            "Max examples per label            36         141\n",
            "Avg examples per label          9.49       11.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "X = np.array(data_frame)\n",
        "print('\\nconfirm results with scikit-learn')\n",
        "k_fold = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
        "for train_index, test_index in k_fold.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "print(X_train,X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03JOY4WLZj3m",
        "outputId": "3b1a174a-7435-4b26-b231-620ff00665e5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "confirm results with scikit-learn\n",
            "TRAIN: [   0    1    2 ... 2683 2684 2685] TEST: [  16   18   20   27   28   29   56   66   90  110  130  134  145  148\n",
            "  174  176  181  182  196  212  216  219  230  234  235  241  245  248\n",
            "  263  264  275  295  298  311  326  329  339  365  372  375  382  404\n",
            "  426  427  469  471  484  528  545  579  581  589  599  603  604  609\n",
            "  614  629  642  648  684  685  695  703  709  720  725  729  737  742\n",
            "  756  766  806  807  813  815  822  823  835  837  839  840  855  869\n",
            "  871  880  886  887  892  897  898  915  929  942  950  952  957  961\n",
            "  969  970  982  992 1002 1018 1025 1051 1055 1058 1059 1091 1092 1094\n",
            " 1104 1118 1122 1149 1167 1172 1175 1189 1194 1198 1206 1213 1215 1227\n",
            " 1235 1256 1263 1268 1272 1283 1284 1303 1320 1333 1338 1349 1361 1376\n",
            " 1377 1385 1392 1400 1407 1425 1431 1432 1434 1437 1499 1531 1534 1538\n",
            " 1542 1553 1559 1566 1572 1600 1625 1631 1656 1657 1688 1705 1709 1723\n",
            " 1724 1734 1754 1756 1770 1781 1787 1788 1794 1795 1800 1807 1831 1843\n",
            " 1846 1852 1870 1907 1914 1924 1932 1935 1962 1973 1985 1987 1994 2010\n",
            " 2014 2035 2047 2050 2085 2090 2107 2108 2118 2134 2140 2143 2144 2159\n",
            " 2174 2175 2180 2192 2215 2219 2234 2239 2241 2247 2248 2257 2265 2271\n",
            " 2281 2295 2302 2308 2309 2311 2320 2324 2330 2333 2342 2343 2351 2357\n",
            " 2366 2386 2404 2406 2438 2439 2444 2446 2460 2467 2477 2497 2498 2505\n",
            " 2515 2518 2529 2546 2552 2554 2568 2569 2585 2597 2599 2608 2618 2640\n",
            " 2658 2662 2672]\n",
            "TRAIN: [   0    2    3 ... 2683 2684 2685] TEST: [   1   11   25   39   42   43   47   48   54   64   68   72   79   87\n",
            "   96   97  104  115  152  168  173  183  191  195  202  210  213  222\n",
            "  256  258  262  277  278  282  299  305  317  330  331  332  351  352\n",
            "  359  361  370  423  437  454  464  470  492  507  510  539  543  563\n",
            "  565  582  588  593  625  628  650  663  675  689  691  692  701  708\n",
            "  719  724  739  755  775  778  788  798  801  814  841  854  879  893\n",
            "  895  920  928  962  964  973 1014 1017 1028 1029 1035 1041 1042 1046\n",
            " 1049 1060 1093 1112 1126 1141 1142 1152 1162 1163 1182 1200 1249 1257\n",
            " 1264 1290 1292 1295 1297 1305 1309 1324 1330 1331 1337 1343 1350 1353\n",
            " 1356 1364 1378 1394 1411 1413 1417 1429 1436 1442 1444 1450 1456 1457\n",
            " 1462 1469 1489 1501 1506 1510 1518 1556 1557 1564 1568 1583 1586 1594\n",
            " 1616 1624 1632 1683 1696 1704 1706 1708 1719 1727 1740 1741 1742 1747\n",
            " 1757 1769 1771 1783 1786 1789 1790 1797 1801 1810 1821 1845 1862 1867\n",
            " 1874 1878 1879 1882 1886 1902 1910 1921 1923 1928 1930 1931 1938 1946\n",
            " 1947 1954 1963 1979 1999 2025 2027 2032 2045 2051 2055 2058 2070 2081\n",
            " 2087 2089 2091 2099 2105 2109 2131 2132 2133 2137 2139 2154 2162 2188\n",
            " 2191 2193 2201 2218 2223 2228 2229 2236 2251 2276 2287 2318 2325 2332\n",
            " 2334 2362 2364 2377 2389 2408 2426 2434 2441 2448 2456 2462 2465 2494\n",
            " 2500 2507 2511 2532 2542 2549 2586 2593 2595 2602 2615 2619 2628 2663\n",
            " 2669 2670 2677]\n",
            "TRAIN: [   0    1    2 ... 2682 2683 2684] TEST: [   4    5    8   12   17   19   24   32   36   65   69   74   94  108\n",
            "  120  133  146  153  158  163  177  184  188  194  199  203  211  220\n",
            "  228  243  252  270  281  287  291  296  306  315  318  342  343  344\n",
            "  345  348  349  362  363  364  366  371  384  406  450  452  463  475\n",
            "  476  481  485  493  500  505  520  530  544  553  555  556  567  573\n",
            "  592  608  612  616  637  638  640  705  707  717  721  728  731  760\n",
            "  762  764  773  774  781  792  794  795  799  803  804  810  819  828\n",
            "  831  832  845  850  857  860  863  876  881  891  922  934  947  980\n",
            "  981  984  988 1003 1009 1024 1033 1036 1048 1061 1065 1079 1080 1084\n",
            " 1088 1090 1111 1114 1115 1123 1136 1137 1139 1143 1151 1156 1158 1161\n",
            " 1166 1177 1179 1196 1199 1214 1216 1231 1241 1242 1243 1248 1281 1298\n",
            " 1302 1323 1339 1348 1358 1369 1370 1393 1398 1399 1404 1422 1440 1525\n",
            " 1532 1546 1558 1563 1570 1597 1602 1605 1611 1619 1622 1628 1636 1642\n",
            " 1644 1699 1712 1717 1722 1725 1731 1739 1748 1749 1752 1779 1806 1820\n",
            " 1829 1839 1841 1856 1859 1877 1905 1916 1918 1920 1941 1945 1961 1971\n",
            " 1976 1998 2017 2037 2068 2079 2097 2098 2101 2128 2150 2163 2173 2194\n",
            " 2207 2208 2211 2214 2246 2272 2278 2291 2297 2317 2337 2382 2384 2387\n",
            " 2390 2391 2398 2405 2421 2429 2457 2459 2471 2479 2480 2491 2501 2508\n",
            " 2553 2559 2560 2564 2576 2578 2581 2588 2590 2613 2614 2634 2641 2649\n",
            " 2650 2654 2685]\n",
            "TRAIN: [   0    1    2 ... 2683 2684 2685] TEST: [  10   31   35   37   60   67   73   81   84   85   95  101  107  123\n",
            "  142  143  147  165  186  217  231  233  255  260  261  274  279  304\n",
            "  307  312  334  335  353  369  373  379  381  394  417  428  449  495\n",
            "  517  518  519  524  537  557  562  566  576  577  601  607  613  621\n",
            "  632  643  661  666  669  679  681  696  697  698  738  744  750  758\n",
            "  761  772  783  808  818  820  824  827  829  838  842  846  847  853\n",
            "  856  861  862  874  883  890  899  912  913  916  924  930  945  948\n",
            "  953  955  966  986  990  998 1000 1005 1010 1026 1050 1053 1057 1068\n",
            " 1076 1082 1085 1098 1099 1101 1109 1110 1132 1144 1145 1148 1150 1153\n",
            " 1174 1178 1186 1191 1217 1226 1236 1274 1276 1288 1296 1328 1347 1352\n",
            " 1360 1363 1379 1396 1416 1418 1426 1427 1445 1452 1464 1467 1472 1476\n",
            " 1486 1493 1523 1540 1554 1565 1576 1581 1589 1591 1620 1629 1630 1661\n",
            " 1668 1673 1690 1711 1715 1743 1746 1750 1761 1776 1784 1791 1792 1799\n",
            " 1815 1830 1835 1837 1844 1853 1854 1861 1876 1885 1901 1904 1906 1934\n",
            " 1940 1943 1948 1975 1980 1983 1991 2029 2040 2060 2075 2093 2095 2104\n",
            " 2127 2149 2151 2152 2160 2166 2179 2212 2221 2222 2225 2227 2242 2264\n",
            " 2275 2293 2298 2300 2301 2305 2313 2315 2336 2352 2358 2368 2371 2375\n",
            " 2392 2393 2396 2413 2415 2420 2424 2427 2430 2453 2478 2487 2514 2516\n",
            " 2519 2522 2523 2524 2525 2530 2533 2562 2572 2574 2589 2594 2610 2617\n",
            " 2622 2625 2667]\n",
            "TRAIN: [   0    1    2 ... 2683 2684 2685] TEST: [   6    7   21   34   38   71  102  106  109  112  124  125  126  131\n",
            "  161  171  178  204  205  218  221  225  226  242  244  267  273  280\n",
            "  301  328  336  337  367  383  385  392  396  398  403  414  415  418\n",
            "  429  431  435  440  456  458  460  491  494  502  512  513  529  533\n",
            "  550  558  564  594  596  598  617  618  619  660  662  670  678  704\n",
            "  715  730  732  734  735  741  743  746  754  767  769  776  779  784\n",
            "  787  811  821  833  864  867  872  896  904  906  907  909  926  941\n",
            "  949  963  983  996  997 1001 1031 1040 1062 1102 1133 1155 1157 1160\n",
            " 1181 1192 1195 1209 1212 1232 1244 1247 1253 1254 1265 1269 1277 1278\n",
            " 1301 1307 1315 1318 1342 1357 1365 1382 1408 1420 1423 1428 1430 1438\n",
            " 1441 1443 1449 1470 1474 1485 1487 1500 1511 1515 1521 1541 1569 1582\n",
            " 1588 1599 1617 1638 1643 1645 1650 1659 1675 1679 1684 1686 1707 1710\n",
            " 1753 1758 1762 1763 1768 1798 1804 1817 1827 1848 1850 1863 1873 1881\n",
            " 1887 1894 1899 1917 1925 1942 1965 1969 1970 1977 1981 1986 1990 1995\n",
            " 2005 2018 2019 2022 2024 2039 2059 2078 2084 2116 2117 2120 2125 2141\n",
            " 2142 2145 2156 2164 2170 2171 2181 2186 2190 2199 2203 2209 2220 2244\n",
            " 2253 2263 2277 2283 2296 2321 2331 2335 2349 2363 2373 2374 2409 2417\n",
            " 2418 2428 2445 2449 2450 2474 2490 2496 2521 2558 2566 2579 2584 2598\n",
            " 2611 2621 2624 2626 2629 2631 2632 2637 2644 2647 2653 2659 2664 2665\n",
            " 2668 2674 2680]\n",
            "TRAIN: [   0    1    2 ... 2682 2684 2685] TEST: [   9   23   33   45   51   75   78  114  118  121  137  139  170  175\n",
            "  179  180  190  214  223  237  247  266  271  303  308  319  321  323\n",
            "  325  338  354  378  387  411  413  421  424  445  447  467  473  474\n",
            "  483  486  489  490  499  501  511  531  571  580  591  605  606  611\n",
            "  615  624  631  633  634  636  654  657  658  665  667  668  693  700\n",
            "  711  718  752  753  786  791  797  800  805  809  816  817  836  844\n",
            "  865  866  870  882  900  932  935  936  937  939  946  951  968  991\n",
            "  995 1004 1011 1019 1021 1022 1056 1063 1067 1070 1077 1081 1117 1146\n",
            " 1173 1183 1185 1203 1205 1218 1238 1246 1251 1252 1260 1285 1308 1313\n",
            " 1314 1322 1395 1448 1451 1468 1484 1494 1496 1497 1504 1507 1533 1548\n",
            " 1549 1561 1567 1573 1579 1592 1595 1601 1608 1610 1637 1640 1641 1646\n",
            " 1653 1654 1665 1667 1670 1671 1678 1685 1693 1695 1703 1726 1728 1736\n",
            " 1737 1745 1755 1760 1764 1766 1767 1772 1778 1796 1808 1809 1814 1819\n",
            " 1822 1824 1825 1838 1858 1865 1866 1869 1872 1913 1915 1927 1939 1951\n",
            " 1959 1982 2001 2004 2006 2011 2030 2043 2046 2049 2054 2061 2064 2071\n",
            " 2074 2088 2092 2094 2100 2112 2115 2121 2122 2123 2135 2157 2172 2183\n",
            " 2189 2205 2232 2245 2252 2266 2282 2285 2286 2290 2299 2316 2340 2359\n",
            " 2370 2422 2431 2437 2440 2447 2466 2489 2492 2493 2513 2517 2543 2544\n",
            " 2547 2551 2555 2582 2600 2601 2604 2633 2639 2642 2646 2648 2671 2675\n",
            " 2678 2681 2683]\n",
            "TRAIN: [   0    1    2 ... 2683 2684 2685] TEST: [   3   22   30   41   52   76   80   82   83  122  136  151  157  159\n",
            "  164  169  172  197  224  229  238  240  276  290  294  300  313  322\n",
            "  324  333  341  358  374  397  416  419  420  425  433  438  439  441\n",
            "  442  461  479  482  496  497  498  514  516  521  525  532  536  538\n",
            "  542  546  548  549  559  561  572  590  641  647  649  672  688  699\n",
            "  706  722  723  749  751  763  768  789  793  802  843  858  859  894\n",
            "  919  940  944  954  960  965  972  976  977  994  999 1007 1012 1016\n",
            " 1030 1038 1039 1044 1054 1064 1074 1075 1108 1116 1124 1128 1129 1130\n",
            " 1131 1164 1165 1176 1197 1201 1202 1208 1220 1223 1234 1250 1259 1273\n",
            " 1286 1304 1311 1319 1321 1329 1346 1355 1359 1371 1380 1402 1405 1421\n",
            " 1435 1455 1461 1466 1471 1488 1491 1503 1509 1517 1519 1528 1535 1544\n",
            " 1551 1560 1574 1578 1585 1593 1609 1626 1627 1649 1652 1664 1669 1674\n",
            " 1682 1697 1702 1733 1775 1777 1803 1805 1812 1813 1816 1832 1836 1883\n",
            " 1884 1890 1891 1893 1896 1903 1909 1911 1936 1937 1956 1972 1978 1989\n",
            " 1996 2002 2012 2026 2072 2077 2082 2083 2096 2111 2124 2136 2148 2153\n",
            " 2158 2161 2168 2169 2178 2198 2200 2224 2235 2237 2243 2250 2255 2256\n",
            " 2259 2261 2274 2279 2288 2292 2303 2312 2326 2341 2344 2346 2350 2365\n",
            " 2385 2414 2423 2432 2451 2455 2458 2472 2481 2488 2499 2503 2509 2510\n",
            " 2536 2540 2541 2545 2556 2561 2570 2573 2575 2577 2587 2609 2612 2616\n",
            " 2623 2627]\n",
            "TRAIN: [   0    1    2 ... 2683 2684 2685] TEST: [  49   53   57   88   92   99  100  116  119  132  135  138  140  141\n",
            "  149  155  156  167  185  192  193  198  206  215  236  246  249  250\n",
            "  253  254  257  259  284  285  292  293  302  357  386  393  399  408\n",
            "  410  430  434  443  444  448  457  477  487  504  508  526  535  547\n",
            "  554  568  569  578  584  585  597  602  622  635  646  652  664  682\n",
            "  683  687  702  712  716  726  736  745  748  757  759  770  782  812\n",
            "  825  868  873  878  884  903  905  908  910  911  931  933  958  971\n",
            "  978  979  987  989 1008 1013 1015 1034 1045 1066 1069 1078 1083 1086\n",
            " 1087 1100 1119 1138 1140 1171 1184 1187 1193 1207 1211 1219 1221 1245\n",
            " 1255 1261 1262 1266 1275 1282 1300 1316 1317 1326 1332 1335 1345 1351\n",
            " 1373 1401 1409 1410 1439 1447 1453 1465 1475 1477 1508 1516 1529 1537\n",
            " 1555 1571 1590 1598 1604 1606 1614 1623 1639 1647 1655 1658 1663 1666\n",
            " 1676 1680 1689 1700 1716 1718 1729 1730 1738 1744 1759 1782 1811 1833\n",
            " 1840 1847 1855 1857 1860 1871 1875 1888 1892 1933 1944 1950 1955 1964\n",
            " 1967 1993 2003 2015 2016 2021 2023 2034 2038 2056 2057 2062 2067 2076\n",
            " 2113 2126 2130 2147 2165 2177 2195 2210 2231 2238 2249 2260 2268 2270\n",
            " 2273 2284 2310 2354 2355 2360 2369 2380 2381 2388 2395 2397 2399 2400\n",
            " 2401 2403 2411 2416 2425 2435 2463 2464 2468 2475 2476 2482 2483 2484\n",
            " 2502 2506 2527 2539 2548 2563 2580 2596 2636 2638 2643 2645 2652 2655\n",
            " 2673 2679]\n",
            "TRAIN: [   1    2    3 ... 2681 2683 2685] TEST: [   0   14   15   26   50   59   61   70   86   91   98  105  111  113\n",
            "  128  129  154  160  162  166  200  227  265  272  289  309  316  320\n",
            "  327  377  388  389  390  400  402  405  407  422  451  462  465  466\n",
            "  468  472  478  488  503  509  522  523  540  560  570  583  586  587\n",
            "  600  610  620  623  626  639  651  659  673  676  677  686  694  713\n",
            "  714  727  740  765  771  790  826  834  848  849  852  885  888  901\n",
            "  943  956  967  975  993 1023 1027 1047 1052 1071 1073 1089 1096 1105\n",
            " 1107 1113 1120 1121 1125 1134 1135 1154 1168 1170 1188 1190 1229 1233\n",
            " 1239 1267 1293 1306 1310 1325 1327 1334 1336 1341 1354 1367 1368 1375\n",
            " 1386 1389 1391 1403 1414 1415 1424 1433 1446 1459 1460 1463 1473 1479\n",
            " 1480 1481 1482 1483 1490 1498 1502 1505 1512 1513 1524 1543 1550 1562\n",
            " 1577 1584 1596 1603 1612 1618 1633 1634 1635 1662 1672 1677 1687 1691\n",
            " 1694 1698 1701 1720 1721 1751 1774 1785 1793 1802 1818 1823 1826 1849\n",
            " 1864 1868 1889 1895 1898 1908 1912 1919 1922 1957 1966 1968 1974 1988\n",
            " 1992 1997 2000 2007 2028 2036 2041 2069 2073 2086 2103 2106 2114 2129\n",
            " 2146 2155 2167 2184 2187 2196 2197 2202 2204 2217 2226 2230 2240 2258\n",
            " 2262 2289 2307 2314 2319 2322 2323 2327 2328 2345 2347 2348 2353 2356\n",
            " 2383 2394 2402 2407 2433 2436 2461 2469 2473 2485 2495 2504 2528 2531\n",
            " 2534 2535 2557 2565 2567 2592 2603 2607 2620 2635 2651 2657 2661 2676\n",
            " 2682 2684]\n",
            "TRAIN: [   0    1    3 ... 2683 2684 2685] TEST: [   2   13   40   44   46   55   58   62   63   77   89   93  103  117\n",
            "  127  144  150  187  189  201  207  208  209  232  239  251  268  269\n",
            "  283  286  288  297  310  314  340  346  347  350  355  356  360  368\n",
            "  376  380  391  395  401  409  412  432  436  446  453  455  459  480\n",
            "  506  515  527  534  541  551  552  574  575  595  627  630  644  645\n",
            "  653  655  656  671  674  680  690  710  733  747  777  780  785  796\n",
            "  830  851  875  877  889  902  914  917  918  921  923  925  927  938\n",
            "  959  974  985 1006 1020 1032 1037 1043 1072 1095 1097 1103 1106 1127\n",
            " 1147 1159 1169 1180 1204 1210 1222 1224 1225 1228 1230 1237 1240 1258\n",
            " 1270 1271 1279 1280 1287 1289 1291 1294 1299 1312 1340 1344 1362 1366\n",
            " 1372 1374 1381 1383 1384 1387 1388 1390 1397 1406 1412 1419 1454 1458\n",
            " 1478 1492 1495 1514 1520 1522 1526 1527 1530 1536 1539 1545 1547 1552\n",
            " 1575 1580 1587 1607 1613 1615 1621 1648 1651 1660 1681 1692 1713 1714\n",
            " 1732 1735 1765 1773 1780 1828 1834 1842 1851 1880 1897 1900 1926 1929\n",
            " 1949 1952 1953 1958 1960 1984 2008 2009 2013 2020 2031 2033 2042 2044\n",
            " 2048 2052 2053 2063 2065 2066 2080 2102 2110 2119 2138 2176 2182 2185\n",
            " 2206 2213 2216 2233 2254 2267 2269 2280 2294 2304 2306 2329 2338 2339\n",
            " 2361 2367 2372 2376 2378 2379 2410 2412 2419 2442 2443 2452 2454 2470\n",
            " 2486 2512 2520 2526 2537 2538 2550 2571 2583 2591 2605 2606 2630 2656\n",
            " 2660 2666]\n",
            "[['235856003' 'liver disease' 'liver disease'\n",
            "  'liver disease-liver disease']\n",
            " ['166717003' 'serum creatinine raised' 'serum creatinine raised'\n",
            "  'serum creatinine raised-serum creatinine raised']\n",
            " ['82991003' 'generalised aches and pains' 'generalised aches and pains'\n",
            "  'generalised aches and pains-generalised aches and pains']\n",
            " ...\n",
            " ['418290006' 'itching' 'intolerable itching'\n",
            "  'itching-intolerable itching']\n",
            " ['30989003' 'knee pain' 'major sorness in knees'\n",
            "  'knee pain-major sorness in knees']\n",
            " ['95891005' 'influenza-like illness'\n",
            "  ' fluey feelings in upper arms, legs'\n",
            "  'influenza-like illness- fluey feelings in upper arms, legs']] [['3877011000036101' 'lipitor' 'lipitor' 'lipitor-lipitor']\n",
            " ['76948002' 'severe pain' 'severe pain' 'severe pain-severe pain']\n",
            " ['449918009' 'cramp in lower leg' 'cramp in lower leg'\n",
            "  'cramp in lower leg-cramp in lower leg']\n",
            " ...\n",
            " ['309087008' 'paraesthesia of foot' 'loss of feeling in toes'\n",
            "  'paraesthesia of foot-loss of feeling in toes']\n",
            " ['76948002' 'severe pain' 'severe pain in my thighs'\n",
            "  'severe pain-severe pain in my thighs']\n",
            " ['57676002' 'arthralgia' 'chronic bone/joint pain'\n",
            "  'arthralgia-chronic bone/joint pain']]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}